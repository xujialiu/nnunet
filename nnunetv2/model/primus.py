import torch
from einops import rearrange
from dynamic_network_architectures.architectures.abstract_arch import (
    AbstractDynamicNetworkArchitectures,
)
from typing import Tuple
import torch
from torch import nn

from dynamic_network_architectures.architectures.abstract_arch import (
    AbstractDynamicNetworkArchitectures,
    test_submodules_loadable,
)
from dynamic_network_architectures.building_blocks.patch_encode_decode import (
    LayerNormNd,
)
from dynamic_network_architectures.initialization.weight_init import InitWeights_He
from einops import rearrange
import numpy as np

import math


class PatchDecode(nn.Module):
    """
    Loosely inspired by SAM decoder
    https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/mask_decoder.py#L53
    """

    def __init__(
        self,
        patch_size: int,
        embed_dim: int,
        out_channels: int,
        norm=LayerNormNd,
        activation=nn.GELU,
    ):
        """
        patch size must be 2^x, so 2, 4, 8, 16, 32, etc. Otherwise we die
        """
        super().__init__()
        assert patch_size > 0
        n = int(math.log2(patch_size))

        assert 2**n == patch_size and n >= 1

        ch = [embed_dim]
        for _ in range(n):
            ch.append(ch[-1] // 2)
        ch.append(out_channels)

        stages = []
        for i in range(n):
            stages.append(
                nn.Sequential(
                    nn.ConvTranspose2d(ch[i], ch[i + 1], kernel_size=2, stride=2),
                    norm(ch[i + 1]),
                    activation(),
                )
            )
        stages.append(nn.Conv2d(ch[-2], ch[-1], kernel_size=1))
        self.decode = nn.Sequential(*stages)

    def forward(self, x):
        """
        Expects input of shape (B, embed_dim, px, py)! This will require you to reshape the output of your transformer!
        """
        return self.decode(x)


class Decoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.deep_supervision = False


class Primus(AbstractDynamicNetworkArchitectures):
    def __init__(
        self,
        embed_dim: int,
        patch_embed_size: int,
        num_classes: int,
        decoder_norm=LayerNormNd,
        decoder_act=nn.GELU,
        dino_encoder=None,
    ):
        """
        Architecture as proposed in the Primus paper (https://arxiv.org/pdf/2503.01835)
        `Primus: Enforcing Attention Usage for 3D Medical Image Segmentation`

        consists of simple patch_embedding, a EVA ViT encoder with a few adatptations and a simple patch decoder.
        """
        super().__init__()

        self.up_projection = PatchDecode(
            patch_embed_size,
            embed_dim,
            num_classes,
            norm=decoder_norm,
            activation=decoder_act,
        )

        # we need to compute the ref_feat_shape for eva
        self.dino_encoder = dino_encoder
        self.decoder = Decoder()
        self.up_projection.apply(InitWeights_He(1e-2))

    def forward(self, x, ret_mask=False):
        indices = 1
        if hasattr(self.dino_encoder, "get_intermediate_layers"):
            feats = self.dino_encoder.get_intermediate_layers(
                x,
                n=indices,
                reshape=True,
                norm=True,
            )
        elif hasattr(self.dino_encoder, "forward_intermediates"):
            _, feats = self.dino_encoder.forward_intermediates(
                x,
                indices=indices,
                norm=True,
                output_fmt="NCHW",
                return_prefix_tokens=False,
            )

        x = feats[0]
        
        dec_out = self.up_projection(x)
        return dec_out

    def compute_conv_feature_map_size(self, input_size):
        raise NotImplementedError("yuck")


class Primus_Multiscale(AbstractDynamicNetworkArchitectures):
    def __init__(
        self,
        embed_dim: int,
        patch_embed_size: int,
        num_classes: int,
        decoder_norm=LayerNormNd,
        decoder_act=nn.GELU,
        dino_encoder=None,
        interaction_indices=[1, 2, 3, 4],
    ):
        """
        We follow a similar design as ViT-adapter, using intermediate layers and concat along channel dimension.
        """
        super().__init__()

        self.up_projection = PatchDecode(
            patch_embed_size,
            embed_dim * len(interaction_indices),
            num_classes,
            norm=decoder_norm,
            activation=decoder_act,
        )

        # we need to compute the ref_feat_shape for eva
        self.dino_encoder = dino_encoder
        self.decoder = Decoder()
        self.up_projection.apply(InitWeights_He(1e-2))
        self.interaction_indices = interaction_indices

    def forward(self, x, ret_mask=False):
        assert x.shape[1] == 1
        x = x.repeat(1, 3, 1, 1)
        hier = self.dino_encoder.get_intermediate_layers(
            x, n=self.interaction_indices, reshape=True
        )
        hier = torch.cat(hier, dim=1)
        dec_out = self.up_projection(hier)
        return dec_out

    def compute_conv_feature_map_size(self, input_size):
        raise NotImplementedError("yuck")
